{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "#进度条库\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#使用ipython魔法方法，将绘制得到的图像直接嵌入在notebook单元格中\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入手部关键点检测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入solution\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "#导入模型\n",
    "hands = mp_hands.Hands(static_image_mode=False,       #是静态图片还是连续视频帧\n",
    "                      max_num_hands=4,                #最多检测几只手\n",
    "                      min_detection_confidence=0.5,   #置信度阈值\n",
    "                      min_tracking_confidence=0.5,    #追踪阈值\n",
    "                      )\n",
    "\n",
    "#导入绘图函数\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个问号查找用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mmp_hands\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstatic_image_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_num_hands\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_detection_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_tracking_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "MediaPipe Hands.\n",
       "\n",
       "MediaPipe Hands processes an RGB image and returns the hand landmarks and\n",
       "handedness (left v.s. right hand) of each detected hand.\n",
       "\n",
       "Note that it determines handedness assuming the input image is mirrored,\n",
       "i.e., taken with a front-facing/selfie camera (\n",
       "https://en.wikipedia.org/wiki/Front-facing_camera) with images flipped\n",
       "horizontally. If that is not the case, use, for instance, cv2.flip(image, 1)\n",
       "to flip the image first for a correct handedness output.\n",
       "\n",
       "Please refer to https://solutions.mediapipe.dev/hands#python-solution-api for\n",
       "usage examples.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Initializes a MediaPipe Hand object.\n",
       "\n",
       "Args:\n",
       "  static_image_mode: Whether to treat the input images as a batch of static\n",
       "    and possibly unrelated images, or a video stream. See details in\n",
       "    https://solutions.mediapipe.dev/hands#static_image_mode.\n",
       "  max_num_hands: Maximum number of hands to detect. See details in\n",
       "    https://solutions.mediapipe.dev/hands#max_num_hands.\n",
       "  min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for hand\n",
       "    detection to be considered successful. See details in\n",
       "    https://solutions.mediapipe.dev/hands#min_detection_confidence.\n",
       "  min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the\n",
       "    hand landmarks to be considered tracked successfully. See details in\n",
       "    https://solutions.mediapipe.dev/hands#min_tracking_confidence.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\32147\\appdata\\roaming\\python\\python37\\site-packages\\mediapipe\\python\\solutions\\hands.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mp_hands.Hands?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两个问号查找源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mmp_hands\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstatic_image_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_num_hands\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_detection_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_tracking_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m        \n",
       "\u001b[1;32mclass\u001b[0m \u001b[0mHands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSolutionBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;34m\"\"\"MediaPipe Hands.\n",
       "\n",
       "  MediaPipe Hands processes an RGB image and returns the hand landmarks and\n",
       "  handedness (left v.s. right hand) of each detected hand.\n",
       "\n",
       "  Note that it determines handedness assuming the input image is mirrored,\n",
       "  i.e., taken with a front-facing/selfie camera (\n",
       "  https://en.wikipedia.org/wiki/Front-facing_camera) with images flipped\n",
       "  horizontally. If that is not the case, use, for instance, cv2.flip(image, 1)\n",
       "  to flip the image first for a correct handedness output.\n",
       "\n",
       "  Please refer to https://solutions.mediapipe.dev/hands#python-solution-api for\n",
       "  usage examples.\n",
       "  \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m               \u001b[0mstatic_image_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m               \u001b[0mmax_num_hands\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m               \u001b[0mmin_detection_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m               \u001b[0mmin_tracking_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Initializes a MediaPipe Hand object.\n",
       "\n",
       "    Args:\n",
       "      static_image_mode: Whether to treat the input images as a batch of static\n",
       "        and possibly unrelated images, or a video stream. See details in\n",
       "        https://solutions.mediapipe.dev/hands#static_image_mode.\n",
       "      max_num_hands: Maximum number of hands to detect. See details in\n",
       "        https://solutions.mediapipe.dev/hands#max_num_hands.\n",
       "      min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for hand\n",
       "        detection to be considered successful. See details in\n",
       "        https://solutions.mediapipe.dev/hands#min_detection_confidence.\n",
       "      min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the\n",
       "        hand landmarks to be considered tracked successfully. See details in\n",
       "        https://solutions.mediapipe.dev/hands#min_tracking_confidence.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mbinary_graph_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBINARYPB_FILE_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mside_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m'num_hands'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_num_hands\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mcalculator_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m'ConstantSidePacketCalculator.packet'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mconstant_side_packet_calculator_pb2\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;33m.\u001b[0m\u001b[0mConstantSidePacketCalculatorOptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConstantSidePacket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mbool_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mstatic_image_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m'palmdetectioncpu__TensorsToDetectionsCalculator.min_score_thresh'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmin_detection_confidence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m'handlandmarkcpu__ThresholdingCalculator.threshold'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmin_tracking_confidence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi_hand_landmarks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'multi_handedness'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNamedTuple\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\n",
       "\n",
       "    Args:\n",
       "      image: An RGB image represented as a numpy ndarray.\n",
       "\n",
       "    Raises:\n",
       "      RuntimeError: If the underlying graph throws any error.\n",
       "      ValueError: If the input image is not three channel RGB.\n",
       "\n",
       "    Returns:\n",
       "      A NamedTuple object with two fields: a \"multi_hand_landmarks\" field that\n",
       "      contains the hand landmarks on each detected hand and a \"multi_handedness\"\n",
       "      field that contains the handedness (left v.s. right hand) of the detected\n",
       "      hand.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\32147\\appdata\\roaming\\python\\python37\\site-packages\\mediapipe\\python\\solutions\\hands.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mp_hands.Hands??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理单帧的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理帧函数\n",
    "def process_frame(img):\n",
    "    \n",
    "    #记录该帧开始处理的时间\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #获取图像的宽高\n",
    "    h,w = img.shape[0],img.shape[1]\n",
    "    \n",
    "    \n",
    "    # 水平镜像翻转图像，使得图中的左右手与真实左右手相对应\n",
    "    #参数，1：水平翻转，0：竖直翻转，-1：水平和竖直都翻转\n",
    "    img = cv2.flip(img,1)\n",
    "    # BGR转RGB\n",
    "    img_RGB = cv2.cvtColor(img,cv2.COLOR_BGRA2RGB)\n",
    "\n",
    "    #将RGB图像输入模型。获取预测结果\n",
    "    results = hands.process(img_RGB)\n",
    "    \n",
    "    \n",
    "    if results.multi_hand_landmarks:   #如果检测到手\n",
    "    \n",
    "        handness_str = ''\n",
    "        index_finger_tip_str = ''\n",
    "        for hand_idx in range(len(results.multi_hand_landmarks)):\n",
    "\n",
    "            #获取该手的21个关键点坐标\n",
    "            hand_21 = results.multi_hand_landmarks[hand_idx] \n",
    "\n",
    "            #可视化关键点及骨架连线\n",
    "            mpDraw.draw_landmarks(img,hand_21,connections = mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            #记录左右手信息\n",
    "            temp_handness = results.multi_handedness[hand_idx].classification[0].label\n",
    "    #        handness_str += str(hand_idx) + ':' + temp_handness + ' '       \n",
    "            handness_str += '{}:{} '.format(hand_idx,temp_handness)\n",
    "\n",
    "            #获取手腕根部深度坐标\n",
    "            cz0 = hand_21.landmark[0].z\n",
    "\n",
    "            for i in range(21):   #遍历该手的21个关键点\n",
    "\n",
    "                #获取3D坐标\n",
    "                cx = int(hand_21.landmark[i].x * w)\n",
    "                cy = int(hand_21.landmark[i].y * h)\n",
    "                cz = hand_21.landmark[i].z\n",
    "                depth_z = cz0 - cz\n",
    "\n",
    "                #用圆的半径反映深度大小\n",
    "                radius = int(6 * (1 + depth_z))\n",
    "\n",
    "                if i == 0:    #手腕\n",
    "                    img = cv2.circle(img,(cx,cy),radius * 2,(0,0,255),-1)\n",
    "                if i == 8:   #食指指尖\n",
    "                    img = cv2.circle(img,(cx,cy),radius * 2,(193,184,67),-1)\n",
    "                    index_finger_tip_str += '{}:{} '.format(hand_idx,depth_z)\n",
    "                if i in [1,5,9,13,17]:  #指根\n",
    "                    img = cv2.circle(img,(cx,cy),radius,(19,14,67),-1)\n",
    "                if i in [2,6,10,14,18]:  #第一指节\n",
    "                    img = cv2.circle(img,(cx,cy),radius,(122,164,67),-1)                \n",
    "                if i in [3,7,11,15,19]:  #第二指节\n",
    "                    img = cv2.circle(img,(cx,cy),radius,(12,150,89),-1)                \n",
    "                if i in [4,12,16,20]:  #指尖（除食指指尖）\n",
    "                    img = cv2.circle(img,(cx,cy),radius,(223,155,60),-1)            \n",
    "\n",
    "        scaler = 1\n",
    "        img = cv2.putText(img,handness_str,(25 * scaler,100 * scaler),cv2.FONT_HERSHEY_SIMPLEX,1.25 * scaler,(255,0,0),2 * scaler)\n",
    "        img = cv2.putText(img,index_finger_tip_str,(25 * scaler,150 * scaler),cv2.FONT_HERSHEY_SIMPLEX,1.25 * scaler,(255,0,255),2 * scaler)\n",
    "        \n",
    "        #记录该帧处理完毕的时间\n",
    "        end_time = time.time()\n",
    "        #计算每秒处理图像帧数FPS\n",
    "        FPS = 1/(end_time - start_time)   #FPS大于30，成为实时目标检测算法\n",
    "        \n",
    "        #在图像上写FPS数值，参数依次为：图片、添加的文字、左上角坐标、字体、字体大小、颜色、字体粗细\n",
    "        scaler = 1\n",
    "        img = cv2.putText(img,'FPS '+ str(int(FPS)), (25 * scaler,50 * scaler),cv2.FONT_HERSHEY_SIMPLEX,1.25 * scaler,(255,0,0),2 * scaler)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 视频逐帧处理（模板）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 视频逐帧处理模板\n",
    "## 不需修改任何代码，只需定义process_frame函数即可\n",
    "\n",
    "def generate_video(input_path = './videos/hands.video.mp4'):\n",
    "    filehead = input_path.split('/')[-1]\n",
    "    output_path = 'out_' + filehead\n",
    "    \n",
    "    print('视频开始处理',input_path)\n",
    "    \n",
    "    #获取视频总帧数\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        success,frame = cap.read()\n",
    "        frame_count += 1\n",
    "        if not success:\n",
    "            break\n",
    "    cap.release()\n",
    "    print('视频总帧数为:',frame_count)\n",
    "        \n",
    "        \n",
    "    #设置输出视频参数\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH),cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    #fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "    #fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    out = cv2.VideoWriter(output_path,fourcc,fps,(int(frame_size[0]),int(frame_size[1])))\n",
    "    \n",
    "    \n",
    "    #进度条绑定视频总帧数\n",
    "    with tqdm(total = frame_count - 1) as pbar:\n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                success,frame = cap.read()\n",
    "                if not success:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    frame = process_frame(frame)\n",
    "                except:\n",
    "                    print('error')\n",
    "                    pass\n",
    "                \n",
    "                if success == True:\n",
    "                    #cv2.imshow('Video Processing',frame)\n",
    "                    out.write(frame)\n",
    "                    \n",
    "                    #进度条更新一帧\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                #if cv2.waitKey(1) in [ord('q'),27]:    #按键盘上的q或esc退出（在英文输入法下）\n",
    "                    #break\n",
    "                    \n",
    "        except:\n",
    "                print('中途中断')\n",
    "                pass\n",
    "    cv2.destroyAllWindows()\n",
    "    out.release()\n",
    "    cap.release()\n",
    "    print('视频已保存:',output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频开始处理 ./videos/hands.video.mp4\n",
      "视频总帧数为: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 255/255 [00:07<00:00, 31.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频已保存: out_hands.video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_video(input_path = './videos/hands.video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
